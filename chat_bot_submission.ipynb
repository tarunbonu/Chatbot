{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T10:42:21.074941Z",
     "start_time": "2018-07-13T10:42:17.155887Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import TfidfModel\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T10:42:21.126258Z",
     "start_time": "2018-07-13T10:42:21.076933Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('chat_bot_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T10:42:21.222197Z",
     "start_time": "2018-07-13T10:42:21.129871Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs_q = list(df['question'])\n",
    "docs_a = list(df['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:45:35.216657Z",
     "start_time": "2018-04-02T11:45:34.516938Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding,LSTM,Bidirectional,Input,Lambda,Dense,Flatten\n",
    "from keras.preprocessing.text import Tokenizer,one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model,Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:45:35.220407Z",
     "start_time": "2018-04-02T11:45:35.218100Z"
    }
   },
   "outputs": [],
   "source": [
    "tok = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:45:52.532953Z",
     "start_time": "2018-04-02T11:45:52.514709Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "for i in range(75):\n",
    "    docs.append(docs_q[i] + ' ' +docs_a[i])\n",
    "    \n",
    "docs = docs + docs_q\n",
    "shuffle(docs)\n",
    "\n",
    "tok.fit_on_texts(docs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:45:57.047224Z",
     "start_time": "2018-04-02T11:45:57.041406Z"
    }
   },
   "outputs": [],
   "source": [
    "LENGTH = 174\n",
    "TOTAL_WORD_COUNT = len(tok.word_counts)\n",
    "EMBEDDING_SIZE = 10\n",
    "input_shape = (LENGTH,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:45:57.607508Z",
     "start_time": "2018-04-02T11:45:57.599504Z"
    }
   },
   "outputs": [],
   "source": [
    "training = tok.texts_to_sequences(docs_q)\n",
    "X = pad_sequences(training,maxlen=LENGTH)\n",
    "Y = tok.texts_to_matrix(docs_q)\n",
    "X = {'X1':X}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:45:59.132632Z",
     "start_time": "2018-04-02T11:45:59.127957Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_encoder():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(TOTAL_WORD_COUNT+1,EMBEDDING_SIZE,input_length=LENGTH))\n",
    "    model.add(Bidirectional(LSTM(8,return_sequences=False)))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:45:59.441148Z",
     "start_time": "2018-04-02T11:45:59.430870Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_network():\n",
    "    X1 = Input(shape=input_shape,name='X1')\n",
    "    \n",
    "    encoder = create_encoder()\n",
    "    \n",
    "    x = encoder(X1)\n",
    "#     x = Dense(64)(x)\n",
    "#     x = Dense(128)(x)\n",
    "    x = Dense(TOTAL_WORD_COUNT+1,activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(X1,x)\n",
    "    model.compile(loss='binary_crossentropy',optimizer='rmsprop')\n",
    "    return model,encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:46:04.667474Z",
     "start_time": "2018-04-02T11:46:04.166902Z"
    }
   },
   "outputs": [],
   "source": [
    "model,encoder = create_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:46:05.115878Z",
     "start_time": "2018-04-02T11:46:05.111475Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T13:08:07.485956Z",
     "start_time": "2018-04-01T13:07:36.652753Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x=X,y=Y,epochs=10000,batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T13:08:10.436512Z",
     "start_time": "2018-04-01T13:08:09.418019Z"
    }
   },
   "outputs": [],
   "source": [
    "qs = tok.texts_to_sequences(docs_q)\n",
    "qs_x = pad_sequences(qs,maxlen=LENGTH)\n",
    "qs_queries = encoder.predict(qs_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T13:08:10.442229Z",
     "start_time": "2018-04-01T13:08:10.437992Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "def cos_sim(a,b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T13:08:10.598065Z",
     "start_time": "2018-04-01T13:08:10.443659Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_similar_ques(query):\n",
    "    qs = tok.texts_to_sequences([query])\n",
    "    qs_x = pad_sequences(qs,maxlen=LENGTH)\n",
    "    queries = encoder.predict(qs_x)\n",
    "#     print(queries)\n",
    "    sims = cosine_similarity(qs_queries,queries)\n",
    "#     print(sims)\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "#     print(sims)\n",
    "#     print(sims)\n",
    "    return sims[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T13:08:10.706224Z",
     "start_time": "2018-04-01T13:08:10.599733Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = \"What is the GMAT requirement to get admission to HBS?\"\n",
    "q = get_similar_ques(doc)\n",
    "print(df.loc[q]['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSI approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:46:22.393734Z",
     "start_time": "2018-04-02T11:46:22.390950Z"
    }
   },
   "outputs": [],
   "source": [
    "docs_q = list(df['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:46:24.933780Z",
     "start_time": "2018-04-02T11:46:22.814832Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name = 'wiki.train.tokens'\n",
    "text = open(file_name).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:46:28.032569Z",
     "start_time": "2018-04-02T11:46:27.280917Z"
    }
   },
   "outputs": [],
   "source": [
    "text = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:46:29.796826Z",
     "start_time": "2018-04-02T11:46:28.034752Z"
    }
   },
   "outputs": [],
   "source": [
    "text = [i for i in text if len(i)>5]\n",
    "text = [j for i in text for j in i.split(\".\") ]\n",
    "\n",
    "all_docs = text[1:10000] + [i for i in docs_q for j in range(100)]\n",
    "shuffle(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:46:39.576588Z",
     "start_time": "2018-04-02T11:46:37.629271Z"
    }
   },
   "outputs": [],
   "source": [
    "# stoplist = set('for a of the and to in : ,'.split())\n",
    "# stoplist = ['i','to','the']\n",
    "def get_token(sentence):\n",
    "    return sentence.split()\n",
    "# stoplist = []\n",
    "# texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "#          for document in all_docs]\n",
    "texts = map(get_token,all_docs)\n",
    "\n",
    "# remove words that appear only once\n",
    "# frequency = defaultdict(int)\n",
    "# for text in texts:\n",
    "#     for token in text:\n",
    "#         frequency[token] += 1\n",
    "# texts = [[token for token in text if frequency[token] > 1]\n",
    "#          for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# dictionary.save('temp.dict')  # store the dictionary, for future reference\n",
    "# print(dictionary)\n",
    "texts = map(get_token,all_docs)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "# corpora.MmCorpus.serialize('temp.mm', corpus)  # store to disk, for later use\n",
    "# print(corpus)\n",
    "# tfidf = TfidfModel(corpus,id2word=dictionary)\n",
    "\n",
    "# corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "# lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:46:40.428075Z",
     "start_time": "2018-04-02T11:46:40.414697Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_similar_ques(doc):\n",
    "    vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "#     vec_tfidf = tfidf[vec_bow]\n",
    "#     vec_lsi = lsi[vec_bow] # convert the query to LSI space\n",
    "    vec_lsi = lsi[vec_bow]\n",
    "    # print(vec_lsi)\n",
    "    \n",
    "    \n",
    "    vec_qs = [dictionary.doc2bow(i.lower().split()) for i in docs_q]\n",
    "#     vec_qs_tfidf = tfidf[vec_qs]\n",
    "#     index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "    index = similarities.MatrixSimilarity(lsi[vec_qs])\n",
    "\n",
    "    sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "    # print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples\n",
    "\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "#     print(sims) # print sorted (document number, similarity score) 2-tuples\n",
    "    return sims[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T11:46:42.844015Z",
     "start_time": "2018-04-02T11:46:42.817202Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = \"How can I reach out to HBS alumni for guidance?\"\n",
    "q = get_similar_ques(doc.replace('?',' ?'))\n",
    "df.loc[q]['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSI approach 1 Submitted one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T10:42:23.901862Z",
     "start_time": "2018-07-13T10:42:23.894692Z"
    }
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "for i in range(75):\n",
    "    docs.append(docs_q[i] + ' ' +docs_a[i])\n",
    "    \n",
    "docs = docs + docs_q\n",
    "shuffle(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T10:42:25.521141Z",
     "start_time": "2018-07-13T10:42:24.756114Z"
    }
   },
   "outputs": [],
   "source": [
    "# stoplist = set('for a of the and to in : ,'.split())\n",
    "# stoplist = ['i','to','the']\n",
    "stoplist = []\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in docs]\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 1]\n",
    "         for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# dictionary.save('temp.dict')  # store the dictionary, for future reference\n",
    "# print(dictionary)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "# corpora.MmCorpus.serialize('temp.mm', corpus)  # store to disk, for later use\n",
    "# print(corpus)\n",
    "tfidf = TfidfModel(corpus,id2word=dictionary)\n",
    "\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "# lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=20,onepass=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T10:42:26.060802Z",
     "start_time": "2018-07-13T10:42:26.043967Z"
    }
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-13T10:42:42.087963Z",
     "start_time": "2018-07-13T10:42:41.970720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. We accept applicants with a wide range of test scores.\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=17,onepass=False)\n",
    "\n",
    "doc = \"What is the GMAT requirement to get admission to HBS?\"\n",
    "q = get_similar_ques(doc)\n",
    "print(df.loc[q]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
